{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f248796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f94ba216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d154b8ba",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b65f3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class MotionDataset(Dataset):\n",
    "    def __init__(self, images, instructions, intents, motions, tasks, tokenizer_name, transform=None):\n",
    "        self.images = images\n",
    "        self.instructions = instructions\n",
    "        self.intents = intents\n",
    "        self.motions = motions\n",
    "        self.tasks = tasks.float()  # Ensure tasks are of type Float\n",
    "        self.transform = transform\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        instruction = self.instructions[idx]\n",
    "        intent = self.intents[idx]\n",
    "        motion = self.motions[idx]\n",
    "        task = self.tasks[idx]\n",
    "\n",
    "        # Tokenize text instructions and intents\n",
    "        instruction_tokens = self.tokenizer(instruction, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        intent_tokens = self.tokenizer(intent, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        if self.transform and isinstance(image, (np.ndarray, Image.Image)):\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, instruction_tokens, intent_tokens, motion, task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a2c013",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11bf51f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalTaskModel(nn.Module):\n",
    "    def __init__(self, text_model_name, num_joints, embed_dim):\n",
    "        super(MultimodalTaskModel, self).__init__()\n",
    "        # Text encoder (e.g., BERT)\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        text_config = AutoConfig.from_pretrained(text_model_name)\n",
    "        self.text_embed_dim = text_config.hidden_size\n",
    "\n",
    "        # Image encoder (simple CNN for now)\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # Output size: (16, 112, 112)\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # Output size: (32, 56, 56)\n",
    "            nn.Flatten(),     # Flatten to (32 * 56 * 56 = 100352)\n",
    "            nn.Linear(32 * 56 * 56, embed_dim)  # Correct input size\n",
    "        )\n",
    "\n",
    "        # Motion encoder (fully connected)\n",
    "        self.motion_encoder = nn.Linear(num_joints * 3, embed_dim)\n",
    "\n",
    "        # Fusion layers\n",
    "        # Updated: Adjusting dimensions to include intent features\n",
    "        self.fc_combined = nn.Linear(self.text_embed_dim * 2 + embed_dim * 2, 256)  # *2 for instruction + intent\n",
    "        self.fc_output = nn.Linear(256, num_joints * 3)\n",
    "\n",
    "    def forward(self, image, instruction_tokens, intent_tokens, motion):\n",
    "        # Text processing\n",
    "        instruction_features = self.text_encoder(**instruction_tokens).pooler_output\n",
    "        intent_features = self.text_encoder(**intent_tokens).pooler_output\n",
    "\n",
    "        # Image processing\n",
    "        image_features = self.image_encoder(image)\n",
    "\n",
    "        # Motion processing\n",
    "        motion_features = self.motion_encoder(motion)\n",
    "\n",
    "        # Combine all features (including intent_features)\n",
    "        combined = torch.cat((instruction_features, intent_features, image_features, motion_features), dim=1)\n",
    "        combined = nn.ReLU()(self.fc_combined(combined))\n",
    "        output = self.fc_output(combined)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187fca4a",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c51e40ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, device, num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            # Unpack batch data\n",
    "            images, instruction_tokens, intent_tokens, motions, tasks = batch\n",
    "            \n",
    "            # Move data to device\n",
    "            images = images.to(device)\n",
    "            motions = motions.to(device)\n",
    "            tasks = tasks.to(device)\n",
    "            instruction_tokens = {key: val.squeeze(1).to(device) for key, val in instruction_tokens.items()}\n",
    "            intent_tokens = {key: val.squeeze(1).to(device) for key, val in intent_tokens.items()}\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images, instruction_tokens, intent_tokens, motions)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, tasks)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update running loss\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f539a0",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bfe6db66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.7807264786500198\n",
      "Epoch 2/5, Loss: 0.2851264545550713\n",
      "Epoch 3/5, Loss: 0.25231581582472873\n",
      "Epoch 4/5, Loss: 0.2470300392462657\n",
      "Epoch 5/5, Loss: 0.24392583622382238\n"
     ]
    }
   ],
   "source": [
    "# Load Data (Dummy Example)\n",
    "# Replace this with real preprocessed data\n",
    "num_data = 100\n",
    "images = torch.randn(num_data, 3, 224, 224)  # Example: num_data images (3 channels, 224x224 resolution)\n",
    "instructions = [\"Turn left\"] * num_data  # Replace with actual instructions\n",
    "intents = [\"Avoid obstacle\"] * num_data  # Replace with actual intents\n",
    "motions = torch.randn(num_data, 360 * 3)  # Example: 360 joints, 3 coordinates each\n",
    "tasks = torch.randint(0, 2, (num_data, 360 * 3))  # Regression task for joint positions\n",
    "\n",
    "# Prepare Dataset and DataLoader\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "tokenizer_name = \"bert-base-uncased\"\n",
    "dataset = MotionDataset(images, instructions, intents, motions, tasks, tokenizer_name, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Initialize Model, Criterion, and Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultimodalTaskModel(\"bert-base-uncased\", 360, 128).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the Model\n",
    "train_model(model, dataloader, criterion, optimizer, device, num_epochs=5)\n",
    "\n",
    "# Save the Model\n",
    "torch.save(model.state_dict(), \"multimodal_task_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4982, 0.6280, 0.3798,  ..., 0.4935, 0.5553, 0.4848]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[0.49816364 0.6280182  0.3797944  ... 0.49352846 0.55528903 0.48482388]]\n",
      "[[0.49816364 0.6280182  0.3797944 ]\n",
      " [0.44454357 0.41951203 0.46397445]\n",
      " [0.39908454 0.5871437  0.41975582]\n",
      " ...\n",
      " [0.48809758 0.43658128 0.51777166]\n",
      " [0.35319674 0.3513166  0.5044269 ]\n",
      " [0.49352846 0.55528903 0.48482388]]\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model weights\n",
    "model.load_state_dict(torch.load(\"multimodal_task_model.pth\", weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# Inference\n",
    "# Replace this with real data\n",
    "new_image = torch.randn(1, 3, 224, 224)  # Example: single image (3 channels, 224x224 resolution)\n",
    "current_joint_positions = np.loadtxt(\"joint_positions.csv\", delimiter=\",\")  # Load joint positions from file\n",
    "new_instruction = \"Turn right\"  # Replace with actual instruction\n",
    "new_intent = \"Avoid obstacle\"  # Replace with actual intent\n",
    "\n",
    "# Tokenize text instructions and intents\n",
    "new_instruction_tokens = dataset.tokenizer(new_instruction, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "new_intent_tokens = dataset.tokenizer(new_intent, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "output = model(new_image.to(device), new_instruction_tokens, new_intent_tokens, torch.tensor(current_joint_positions).float().to(device))\n",
    "print(output)\n",
    "\n",
    "# Convert output to numpy array\n",
    "output_np = output.detach().cpu().numpy()\n",
    "print(output_np)\n",
    "\n",
    "# Convert output to joint positions\n",
    "joint_positions = output_np.reshape(360, 3)\n",
    "print(joint_positions)\n",
    "\n",
    "# Save joint positions to file\n",
    "np.savetxt(\"joint_positions.csv\", joint_positions, delimiter=\",\")\n",
    "\n",
    "# Load joint positions from file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
